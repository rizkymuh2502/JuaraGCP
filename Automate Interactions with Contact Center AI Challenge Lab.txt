Enable Dialogflow API
Clone the Speech Analysis Framework source repository from the Cloud Shell
	git clone https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis.git

TASK #1: CREATE BUCKET
NAVIGATION >> STORAGE >> CREATE BUCKET
Region : us-central1


TASK #2: CREATE A CLOUD FUNCION
NAVIGATION >> CLOUD FUNCTION >> CREATE A NEW FUNCTION
Name 			: saf-longrun-job-func
Region			: us-central1
Trigger Resource	: Cloud Storage
Event Type		: Finalize/Create
Bucket			: <YOUR BUCKET NAME>
Function to execute	: safLongRunJobFunc
Runtime			: nodejs8
Open cloud editor >> open INDEX.JS and PACKAGE.JSON in directory dataflow-contact-center-speech-analysis/saf-longrun-job-dataflow
Replace the INDEX.JS and PACKAGE.JSON cloud in replace with index.js and package.json in the dataflow-contact-center-speech-analysis/saf-longrun-job-dataflow directory
DEPLOY

TASK #3 : CREATE A BIGQUERY
NAVIGATION >> BIGQUERY
SELECT YOUR NAME PROJECT >> CREATE DATASET
Dataset ID 		: lab
Create

TASK #4 : CREATE CLOUD PUB/SUB TOPIC
NAVIGATION >> PUB/SUB >> TOPIC >> CREATE TOPIC
Topic ID		: long-running
Create

TASK #5 : CREATE A CLOUD STORAGE BUCKET FOR STAGING CONTENTS
NAVIGATION >> CLOUD STORAGE >> CLICK YOUT BUCKET >> CREATE FOLDER
Name Folder		: DFaudio

TASK $6 : DEPLIY A CLOUD DATAFLOW PIPELINE
OPEN CLOUD SHELL, RUN FOLLOWING COMMAND TO DEPLOY THE DATAFLOW PIPELINE
	cd dataflow-contact-center-speech-analysis/saf-longrun-job-dataflow

	python -m virtualenv env -p python3
	source env/bin/activate
	pip install apache-beam[gcp]
	pip install dateparser

	export PROJECT_ID=<YOUR PROJECT NAME>
	export TOPIC_NAME=<YOUR TOPIC NAME>
	export BUCKET_NAME=<YOUR BUCKET NAME>/DFaudio
	export DATASET_NAME=<YOUR DATASET NAME>
	export TABLE_NAME=transcript

	python3 saflongrunjobdataflow.py --project=$PROJECT_ID --input_topic=projects/$PROJECT_ID/topics/$TOPIC_NAME --runner=DataflowRunner --region=us-central1 --temp_location=gs://$BUCKET_NAME/tmp --output_bigquery=$DATASET_NAME.$TABLE_NAME --requirements_file="requirements.txt"

TASK #7 : UPLOAD SAMPLE AUDIO FILES FOR PROCESSING
IN THE CLOUD SHELL, RUN THE FOLLOWING COMMAND TO UPLOAD THE SAMPLE AUDIO FILES INTO YOUR AUDIO UPLOAD BUCKET

# mono flac audio sample
	gsutil -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:false -h x-goog-meta-pubsubtopicname:$TOPIC_NAME -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_mono.flac gs://$BUCKET_NAME

# stereo wav audio sample
	gsutil -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:true -h x-goog-meta-pubsubtopicname:$TOPIC_NAME -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_stereo.wav gs://$BUCKET_NAME

TASK #8: RUN A DATA LOSS PREVENTION JOB
NAVIGATION >> BIGQUERY
Select the table generated by the Dataflow pipeline
Click on More >> Query Settings
Assign a Table name, e.g. copied , then click Save
Run the following SQL Query :

SELECT * FROM `[YOUR_PROJECT_ID].[DATASET_NAME].[TABLE]`

Select the copied table, then click on EXPORT > Scan with DLP
In the Create job or job trigger pane, assign a Job ID and then click CREATE
Click CONFIRM CREATE